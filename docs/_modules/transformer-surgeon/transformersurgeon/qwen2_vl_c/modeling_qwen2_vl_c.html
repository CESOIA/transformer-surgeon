

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>transformer-surgeon.transformersurgeon.qwen2_vl_c.modeling_qwen2_vl_c &mdash; transformer-surgeon 0.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />

  
    <link rel="canonical" href="https://CESOIA.github.io/transformer-surgeon/_modules/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c.html" />
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=938c9ccc"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            transformer-surgeon
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/index.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">transformer-surgeon</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">transformer-surgeon.transformersurgeon.qwen2_vl_c.modeling_qwen2_vl_c</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for transformer-surgeon.transformersurgeon.qwen2_vl_c.modeling_qwen2_vl_c</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1">#</span>
<span class="c1"># Copyright 2025 The CESOIA project team, Politecnico di Torino and King Abdullah University of Science and Technology. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This file is based on HuggingFace Transformers Qwen2.5 implementation:</span>
<span class="c1"># Source repository: https://github.com/huggingface/transformers</span>
<span class="c1"># Source file: https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py</span>
<span class="c1"># Commit: 1d742644c09928d6d596c56eae2ffcc8e303be6e</span>
<span class="c1"># Retrieved on 2025-09-04</span>
<span class="c1">#</span>
<span class="c1"># Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This code is based on EleutherAI&#39;s GPT-NeoX library and the GPT-NeoX</span>
<span class="c1"># and OPT implementations in this library. It has been modified from its</span>
<span class="c1"># original forms to accommodate minor architectural differences compared</span>
<span class="c1"># to GPT-NeoX and OPT used by the Meta AI team that trained the model.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;PyTorch Qwen2-VL model.&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.checkpoint</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">LayerNorm</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># from ...activations import ACT2FN</span>
<span class="c1"># from ...cache_utils import Cache, DynamicCache</span>
<span class="c1"># from ...generation import GenerationMixin</span>
<span class="c1"># from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask</span>
<span class="c1"># from ...modeling_flash_attention_utils import FlashAttentionKwargs</span>
<span class="c1"># from ...modeling_layers import GradientCheckpointingLayer</span>
<span class="c1"># from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput</span>
<span class="c1"># from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update</span>
<span class="c1"># from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel</span>
<span class="c1"># from ...processing_utils import Unpack</span>
<span class="c1"># from ...utils import (</span>
<span class="c1">#     TransformersKwargs,</span>
<span class="c1">#     auto_docstring,</span>
<span class="c1">#     can_return_tuple,</span>
<span class="c1">#     is_torchdynamo_compiling,</span>
<span class="c1">#     logging,</span>
<span class="c1"># )</span>
<span class="c1"># from ...utils.deprecation import deprecate_kwarg</span>
<span class="c1"># from ..qwen2.modeling_qwen2 import (</span>
<span class="c1">#     Qwen2RMSNorm,</span>
<span class="c1"># )</span>
<span class="c1"># from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig, Qwen2VLVisionConfig</span>
<span class="c1"># -------------</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.activations</span><span class="w"> </span><span class="kn">import</span> <span class="n">ACT2FN</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">auto_docstring</span><span class="p">,</span>
    <span class="n">logging</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.utils.deprecation</span><span class="w"> </span><span class="kn">import</span> <span class="n">deprecate_kwarg</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.qwen2.modeling_qwen2</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Qwen2RMSNorm</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.configuration_qwen2_vl_c</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Qwen2VLConfigCompress</span><span class="p">,</span>
    <span class="n">Qwen2VLTextConfigCompress</span><span class="p">,</span>
    <span class="n">Qwen2VLVisionConfigCompress</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.qwen2_vl.modeling_qwen2_vl</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Qwen2VLRotaryEmbedding</span><span class="p">,</span>
    <span class="n">VisionRotaryEmbedding</span><span class="p">,</span>
    <span class="n">PatchEmbed</span><span class="p">,</span>
    <span class="n">PatchMerger</span><span class="p">,</span>
    <span class="n">VisionAttention</span><span class="p">,</span>
    <span class="n">Qwen2VLVisionBlock</span><span class="p">,</span>
    <span class="n">Qwen2MLP</span><span class="p">,</span>
    <span class="n">Qwen2VLAttention</span><span class="p">,</span>
    <span class="n">Qwen2VLDecoderLayer</span><span class="p">,</span>
    <span class="n">Qwen2VLPreTrainedModel</span><span class="p">,</span>
    <span class="n">Qwen2VisionTransformerPretrainedModel</span><span class="p">,</span>
    <span class="n">Qwen2VLTextModel</span><span class="p">,</span>
    <span class="n">Qwen2VLModel</span><span class="p">,</span>
    <span class="n">Qwen2VLForConditionalGeneration</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">..utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearCompressed</span><span class="p">,</span>
    <span class="n">get_validated_dict_value</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @dataclass</span>
<span class="c1"># @auto_docstring(</span>
<span class="c1">#     custom_intro=&quot;&quot;&quot;</span>
<span class="c1">#     Base class for Llava outputs, with hidden states and attentions.</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1"># )</span>
<span class="c1"># class Qwen2VLModelOutputWithPast(ModelOutput):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @dataclass</span>
<span class="c1"># @auto_docstring(</span>
<span class="c1">#     custom_intro=&quot;&quot;&quot;</span>
<span class="c1">#     Base class for Qwen2VL causal language model (or autoregressive) outputs.</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1"># )</span>
<span class="c1"># class Qwen2VLCausalLMOutputWithPast(ModelOutput):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
    
<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2VLRotaryEmbedding(nn.Module):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># Copied from transformers.models.llama.modeling_llama.rotate_half</span>
<span class="c1"># def rotate_half(x):</span>
<span class="c1">#     &quot;&quot;&quot;Rotates half the hidden dims of the input.&quot;&quot;&quot;</span>
<span class="c1">#     x1 = x[..., : x.shape[-1] // 2]</span>
<span class="c1">#     x2 = x[..., x.shape[-1] // 2 :]</span>
<span class="c1">#     return torch.cat((-x2, x1), dim=-1)</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):</span>
<span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def apply_rotary_pos_emb_vision(</span>
<span class="c1">#     q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor</span>
<span class="c1"># ) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
<span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class VisionRotaryEmbedding(nn.Module):</span>
<span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class PatchEmbed(nn.Module):</span>
<span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># NOTE: this could potentially be compressed given the presence of two linear layers</span>
<span class="c1"># original ---&gt;</span>
<span class="c1"># class PatchMerger(nn.Module):</span>
<span class="c1">#     def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.hidden_size = context_dim * (spatial_merge_size**2)</span>
<span class="c1">#         self.ln_q = LayerNorm(context_dim, eps=1e-6)</span>
<span class="c1">#         self.mlp = nn.Sequential(</span>
<span class="c1">#             nn.Linear(self.hidden_size, self.hidden_size),</span>
<span class="c1">#             nn.GELU(),</span>
<span class="c1">#             nn.Linear(self.hidden_size, dim),</span>
<span class="c1">#         )</span>

<span class="c1">#     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</span>
<span class="c1">#         x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))</span>
<span class="c1">#         return x</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class VisionMlp(nn.Module):</span>
<span class="c1"># def __init__(self, dim: int, hidden_dim: int, hidden_act: str) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.fc1 = nn.Linear(dim, hidden_dim)</span>
<span class="c1">#         self.act = ACT2FN[hidden_act]</span>
<span class="c1">#         self.fc2 = nn.Linear(hidden_dim, dim)</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">VisionMlpCompress</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_act</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_fc1</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_up&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_fc2</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_down&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_fc1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">hidden_act</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_fc2</span><span class="p">)</span>
<span class="c1"># &lt;------------- CESOIA modifications</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># Copied from transformers.models.llama.modeling_llama.repeat_kv</span>
<span class="c1"># def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def eager_attention_forward(</span>
<span class="c1">#     module: nn.Module,</span>
<span class="c1">#     query: torch.Tensor,</span>
<span class="c1">#     key: torch.Tensor,</span>
<span class="c1">#     value: torch.Tensor,</span>
<span class="c1">#     attention_mask: Optional[torch.Tensor],</span>
<span class="c1">#     scaling: float,</span>
<span class="c1">#     dropout: float = 0.0,</span>
<span class="c1">#     **kwargs,</span>
<span class="c1"># ):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class VisionAttention(nn.Module):</span>
<span class="c1">#     def __init__(self, config: Qwen2VLVisionConfig) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">VisionAttentionCompress</span><span class="p">(</span><span class="n">VisionAttention</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLVisionConfigCompress</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VisionAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># needed for eager attention</span>
<span class="c1"># original ---&gt;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
<span class="c1"># -------------</span>
        <span class="n">rank_qkv</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_qkv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">rank_proj</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_out&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="n">rank_qkv</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="n">rank_proj</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     cu_seqlens: torch.Tensor,</span>
    <span class="c1">#     rotary_pos_emb: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ) -&gt; torch.Tensor:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2VLVisionBlock(GradientCheckpointingLayer):</span>
<span class="c1">#     def __init__(self, config, attn_implementation: str = &quot;sdpa&quot;) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLVisionBlockCompress</span><span class="p">(</span><span class="n">Qwen2VLVisionBlock</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sdpa&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLVisionBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_ratio</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">VisionAttentionCompress</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">)</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.mlp = VisionMlp(dim=config.embed_dim, hidden_dim=mlp_hidden_dim, hidden_act=config.hidden_act)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">VisionMlpCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">hidden_act</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">)</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     cu_seqlens: torch.Tensor,</span>
    <span class="c1">#     rotary_pos_emb: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ) -&gt; torch.Tensor:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># Copied from transformers.models.qwen2.modeling_qwen2.Qwen2MLP</span>
<span class="c1"># class Qwen2MLP(nn.Module):</span>
<span class="c1">#     def __init__(self, config):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2MLPCompress</span><span class="p">(</span><span class="n">Qwen2MLP</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)</span>
        <span class="c1"># self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)</span>
        <span class="c1"># self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_gate</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_gate&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_up</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_up&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_down</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_down&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_gate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_up</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_down</span><span class="p">)</span>
<span class="c1"># &lt;------------- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def forward(self, x):</span>
    <span class="c1">#     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))</span>
    <span class="c1">#     return down_proj</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2VLAttention(nn.Module):</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     Multi-headed attention from &#39;Attention Is All You Need&#39; paper. Modified to use sliding window attention: Longformer</span>
<span class="c1">#     and &quot;Generating Long Sequences with Sparse Transformers&quot;.</span>
<span class="c1">#     &quot;&quot;&quot;</span>

<span class="c1">#     def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLAttentionCompress</span><span class="p">(</span><span class="n">Qwen2VLAttention</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLTextConfigCompress</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        <span class="k">if</span> <span class="n">layer_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Instantiating </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> without passing `layer_idx` is not recommended and will &quot;</span>
                <span class="s2">&quot;to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` &quot;</span>
                <span class="s2">&quot;when creating this class.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rope_scaling</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rope_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;hidden_size must be divisible by num_heads (got `hidden_size`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and `num_heads`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)</span>
        <span class="c1"># self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)</span>
        <span class="c1"># self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)</span>
        <span class="c1"># self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_q</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_q&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_k</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_k&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_v</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_v&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rank_o</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_out&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_q</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_v</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rank_o</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sliding_window</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;sliding_attention&quot;</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">Qwen2VLRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># @deprecate_kwarg(&quot;past_key_value&quot;, new_name=&quot;past_key_values&quot;, version=&quot;4.58&quot;)</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     output_attentions: bool = False,</span>
    <span class="c1">#     use_cache: bool = False,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC</span>
    <span class="c1">#     **kwargs: Unpack[FlashAttentionKwargs],</span>
    <span class="c1"># ) -&gt; tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2VLDecoderLayer(GradientCheckpointingLayer):</span>
<span class="c1">#     def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLDecoderLayerCompress</span><span class="p">(</span><span class="n">Qwen2VLDecoderLayer</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLTextConfigCompress</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_sliding_window</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">!=</span> <span class="s2">&quot;flash_attention_2&quot;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Sliding Window Attention is enabled but not implemented for `</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span><span class="si">}</span><span class="s2">`; &quot;</span>
                <span class="s2">&quot;unexpected results may be encountered.&quot;</span>
            <span class="p">)</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.self_attn = Qwen2VLAttention(config, layer_idx)</span>

        <span class="c1"># self.mlp = Qwen2MLP(config)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">Qwen2VLAttentionCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Qwen2MLPCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># @deprecate_kwarg(&quot;past_key_value&quot;, new_name=&quot;past_key_values&quot;, version=&quot;4.58&quot;)</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[tuple[torch.Tensor]] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = False,</span>
    <span class="c1">#     use_cache: Optional[bool] = False,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC</span>
    <span class="c1">#     **kwargs: Unpack[FlashAttentionKwargs],</span>
    <span class="c1"># ) -&gt; tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        
<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2VLPreTrainedModel(PreTrainedModel):</span>
<span class="c1">#     config: Qwen2VLConfig</span>
<span class="c1">#     base_model_prefix = &quot;model&quot;</span>
<span class="c1">#     supports_gradient_checkpointing = True</span>
<span class="c1">#     _no_split_modules = [&quot;Qwen2VLDecoderLayer&quot;, &quot;Qwen2VLVisionBlock&quot;]</span>
<span class="c1">#     _skip_keys_device_placement = &quot;past_key_values&quot;</span>
<span class="c1">#     _supports_flash_attn = True</span>
<span class="c1">#     _supports_sdpa = True</span>

<span class="c1">#     _can_compile_fullgraph = True</span>
<span class="c1">#     _supports_attention_backend = True</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2VLPreTrainedModelCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLPreTrainedModelCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLPreTrainedModelCompress</span><span class="p">(</span><span class="n">Qwen2VLPreTrainedModel</span><span class="p">):</span>
<div class="viewcode-block" id="Qwen2VLPreTrainedModelCompress.config">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLPreTrainedModelCompress.config">[docs]</a>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLConfigCompress</span></div>

<div class="viewcode-block" id="Qwen2VLPreTrainedModelCompress._no_split_modules">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLPreTrainedModelCompress._no_split_modules">[docs]</a>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2VLDecoderLayerCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2VLVisionBlockCompress&quot;</span><span class="p">]</span></div>
</div>

<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):</span>
<span class="c1">#     config: Qwen2VLVisionConfig</span>
<span class="c1">#     _no_split_modules = [&quot;Qwen2VLVisionBlock&quot;]</span>

<span class="c1">#     def __init__(self, config) -&gt; None:</span>
<span class="c1">#         super().__init__(config)</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VisionTransformerPretrainedModelCompress</span><span class="p">(</span><span class="n">Qwen2VisionTransformerPretrainedModel</span><span class="p">,</span> <span class="n">Qwen2VLPreTrainedModelCompress</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLVisionConfigCompress</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2VLVisionBlockCompress&quot;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLPreTrainedModelCompress</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_merge_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">spatial_merge_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span>
            <span class="n">patch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">temporal_patch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">temporal_patch_size</span><span class="p">,</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">VisionRotaryEmbedding</span><span class="p">(</span><span class="n">head_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.blocks = nn.ModuleList([Qwen2VLVisionBlock(config) for _ in range(config.depth)])</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Qwen2VLVisionBlockCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">)])</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merger</span> <span class="o">=</span> <span class="n">PatchMerger</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">context_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">spatial_merge_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">spatial_merge_size</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_dtype(self) -&gt; torch.dtype:</span>
    <span class="c1">#     return self.blocks[0].mlp.fc2.weight.dtype</span>

    <span class="c1"># def get_device(self) -&gt; torch.device:</span>
    <span class="c1">#     return self.blocks[0].mlp.fc2.weight.device</span>

    <span class="c1"># def rot_pos_emb(self, grid_thw):</span>
    <span class="c1">#     [...]</span>
        
    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     grid_thw: torch.Tensor,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ) -&gt; torch.Tensor:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2VLTextModel(Qwen2VLPreTrainedModel):</span>
<span class="c1">#     config: Qwen2VLTextConfig</span>

<span class="c1">#     def __init__(self, config: Qwen2VLTextConfig):</span>
<span class="c1">#         super().__init__(config)</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2VLTextModelCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLTextModelCompress</span><span class="p">(</span><span class="n">Qwen2VLTextModel</span><span class="p">,</span> <span class="n">Qwen2VLPreTrainedModelCompress</span><span class="p">):</span>
<div class="viewcode-block" id="Qwen2VLTextModelCompress.config">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.config">[docs]</a>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLTextConfigCompress</span></div>


    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLTextConfigCompress</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLTextModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
<div class="viewcode-block" id="Qwen2VLTextModelCompress.padding_idx">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.padding_idx">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span></div>

<div class="viewcode-block" id="Qwen2VLTextModelCompress.vocab_size">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.vocab_size">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span></div>


<div class="viewcode-block" id="Qwen2VLTextModelCompress.embed_tokens">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.embed_tokens">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span></div>

<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.layers = nn.ModuleList(</span>
        <span class="c1">#     [Qwen2VLDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]</span>
        <span class="c1"># )</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2VLTextModelCompress.layers">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.layers">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">Qwen2VLDecoderLayerCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)]</span>
        <span class="p">)</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>
<div class="viewcode-block" id="Qwen2VLTextModelCompress._attn_implementation">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress._attn_implementation">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span></div>

<div class="viewcode-block" id="Qwen2VLTextModelCompress.norm">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.norm">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span></div>

<div class="viewcode-block" id="Qwen2VLTextModelCompress.rotary_emb">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.rotary_emb">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">Qwen2VLRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span></div>

<div class="viewcode-block" id="Qwen2VLTextModelCompress.has_sliding_layers">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.has_sliding_layers">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_sliding_layers</span> <span class="o">=</span> <span class="s2">&quot;sliding_attention&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">layer_types</span></div>


<div class="viewcode-block" id="Qwen2VLTextModelCompress.gradient_checkpointing">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLTextModelCompress.gradient_checkpointing">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span></div>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     inputs_embeds: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     use_cache: Optional[bool] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = None,</span>
    <span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
    <span class="c1">#     return_dict: Optional[bool] = None,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     **kwargs: Unpack[FlashAttentionKwargs],</span>
    <span class="c1"># ) -&gt; Union[tuple, BaseModelOutputWithPast]:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2VLModel(Qwen2VLPreTrainedModel):</span>
<span class="c1">#     base_model_prefix = &quot;&quot;</span>
<span class="c1">#     _checkpoint_conversion_mapping = {&quot;^model&quot;: &quot;language_model&quot;}</span>
<span class="c1">#     # Reference: fix gemma3 grad acc #37208</span>
<span class="c1">#     accepts_loss_kwargs = False</span>

<span class="c1">#     def __init__(self, config: Qwen2VLConfig):</span>
<span class="c1">#         super().__init__(config)</span>
<span class="c1">#         self.visual = Qwen2VisionTransformerPretrainedModel._from_config(config.vision_config)</span>
<span class="c1">#         self.language_model = Qwen2VLTextModel._from_config(config.text_config)</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2VLModelCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLModelCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLModelCompress</span><span class="p">(</span><span class="n">Qwen2VLModel</span><span class="p">,</span> <span class="n">Qwen2VLPreTrainedModelCompress</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2VLConfigCompress</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLPreTrainedModelCompress</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<div class="viewcode-block" id="Qwen2VLModelCompress.visual">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLModelCompress.visual">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">visual</span> <span class="o">=</span> <span class="n">Qwen2VisionTransformerPretrainedModelCompress</span><span class="o">.</span><span class="n">_from_config</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vision_config</span><span class="p">)</span></div>

<div class="viewcode-block" id="Qwen2VLModelCompress.language_model">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLModelCompress.language_model">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span> <span class="o">=</span> <span class="n">Qwen2VLTextModelCompress</span><span class="o">.</span><span class="n">_from_config</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="p">)</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>
<div class="viewcode-block" id="Qwen2VLModelCompress.rope_deltas">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLModelCompress.rope_deltas">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">rope_deltas</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># cache rope_deltas here</span></div>


        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_input_embeddings(self):</span>
    <span class="c1">#     return self.language_model.get_input_embeddings()</span>

    <span class="c1"># def set_input_embeddings(self, value):</span>
    <span class="c1">#     self.language_model.set_input_embeddings(value)</span>

    <span class="c1"># def set_decoder(self, decoder):</span>
    <span class="c1">#     self.language_model = decoder</span>

    <span class="c1"># def get_decoder(self):</span>
    <span class="c1">#     return self.language_model</span>

    <span class="c1"># def get_rope_index(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     image_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     video_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1"># ) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># def get_video_features(</span>
    <span class="c1">#     self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None</span>
    <span class="c1"># ):</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># def get_placeholder_mask(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: torch.LongTensor,</span>
    <span class="c1">#     inputs_embeds: torch.FloatTensor,</span>
    <span class="c1">#     image_features: torch.FloatTensor = None,</span>
    <span class="c1">#     video_features: torch.FloatTensor = None,</span>
    <span class="c1"># ):</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: torch.LongTensor = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     inputs_embeds: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     use_cache: Optional[bool] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = None,</span>
    <span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
    <span class="c1">#     return_dict: Optional[bool] = None,</span>
    <span class="c1">#     pixel_values: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     pixel_values_videos: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     image_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     video_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     rope_deltas: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     **kwargs: Unpack[TransformersKwargs],</span>
    <span class="c1"># ) -&gt; Union[tuple, Qwen2VLModelOutputWithPast]:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):</span>
<span class="c1">#     _checkpoint_conversion_mapping = {</span>
<span class="c1">#         &quot;^visual&quot;: &quot;model.visual&quot;,</span>
<span class="c1">#         r&quot;^model(?!\.(language_model|visual))&quot;: &quot;model.language_model&quot;,</span>
<span class="c1">#     }</span>
<span class="c1">#     _tied_weights_keys = [&quot;lm_head.weight&quot;]</span>

<span class="c1">#     def __init__(self, config):</span>
<span class="c1">#         super().__init__(config)</span>
<span class="c1">#         self.model = Qwen2VLModel(config)</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2VLForConditionalGenerationCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLForConditionalGenerationCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2VLForConditionalGenerationCompress</span><span class="p">(</span><span class="n">Qwen2VLForConditionalGeneration</span><span class="p">,</span> <span class="n">Qwen2VLPreTrainedModelCompress</span><span class="p">):</span>
<div class="viewcode-block" id="Qwen2VLForConditionalGenerationCompress.config_class">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLForConditionalGenerationCompress.config_class">[docs]</a>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">Qwen2VLConfigCompress</span></div>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2VLForConditionalGeneration</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<div class="viewcode-block" id="Qwen2VLForConditionalGenerationCompress.model">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLForConditionalGenerationCompress.model">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Qwen2VLModelCompress</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>
<div class="viewcode-block" id="Qwen2VLForConditionalGenerationCompress.lm_head">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_vl_c/modeling_qwen2_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2VLForConditionalGenerationCompress.lm_head">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_input_embeddings(self):</span>
    <span class="c1">#     return self.model.get_input_embeddings()</span>

    <span class="c1"># def set_input_embeddings(self, value):</span>
    <span class="c1">#     self.model.set_input_embeddings(value)</span>

    <span class="c1"># def set_decoder(self, decoder):</span>
    <span class="c1">#     self.model.set_decoder(decoder)</span>

    <span class="c1"># def get_decoder(self):</span>
    <span class="c1">#     return self.model.get_decoder()</span>

    <span class="c1"># def get_video_features(</span>
    <span class="c1">#     self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None</span>
    <span class="c1"># ):</span>
    <span class="c1">#     return self.model.get_video_features(pixel_values_videos, video_grid_thw)</span>

    <span class="c1"># def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):</span>
    <span class="c1">#     return self.model.get_image_features(pixel_values, image_grid_thw)</span>

    <span class="c1"># # Make modules available through conditional class for BC</span>
    <span class="c1"># @property</span>
    <span class="c1"># def language_model(self):</span>
    <span class="c1">#     return self.model.language_model</span>

    <span class="c1"># @property</span>
    <span class="c1"># def visual(self):</span>
    <span class="c1">#     return self.model.visual</span>

    <span class="c1"># @can_return_tuple</span>
    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: torch.LongTensor = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     inputs_embeds: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     labels: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     use_cache: Optional[bool] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = None,</span>
    <span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
    <span class="c1">#     pixel_values: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     pixel_values_videos: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     image_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     video_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     rope_deltas: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     **kwargs: Unpack[TransformersKwargs],</span>
    <span class="c1"># ) -&gt; Union[tuple, Qwen2VLCausalLMOutputWithPast]:</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># def prepare_inputs_for_generation(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids,</span>
    <span class="c1">#     past_key_values=None,</span>
    <span class="c1">#     attention_mask=None,</span>
    <span class="c1">#     inputs_embeds=None,</span>
    <span class="c1">#     cache_position=None,</span>
    <span class="c1">#     position_ids=None,</span>
    <span class="c1">#     use_cache=True,</span>
    <span class="c1">#     pixel_values=None,</span>
    <span class="c1">#     pixel_values_videos=None,</span>
    <span class="c1">#     image_grid_thw=None,</span>
    <span class="c1">#     video_grid_thw=None,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ):</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># def _get_image_nums_and_video_nums(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor],</span>
    <span class="c1">#     inputs_embeds: Optional[torch.Tensor] = None,</span>
    <span class="c1"># ) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
    <span class="c1">#     [...]</span>

    <span class="c1"># def _expand_inputs_for_generation(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     expand_size: int = 1,</span>
    <span class="c1">#     is_encoder_decoder: bool = False,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     **model_kwargs,</span>
    <span class="c1"># ) -&gt; tuple[torch.LongTensor, dict[str, Any]]:</span>
    <span class="c1">#     [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>


<span class="c1"># original ---&gt;</span>
<span class="c1"># __all__ = [&quot;Qwen2VLForConditionalGeneration&quot;, &quot;Qwen2VLModel&quot;, &quot;Qwen2VLPreTrainedModel&quot;, &quot;Qwen2VLTextModel&quot;]</span>
<span class="c1"># -------------</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2VLForConditionalGenerationCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2VLModelCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2VLPreTrainedModelCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2VLTextModelCompress&quot;</span><span class="p">]</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CESOIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>