

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>transformer-surgeon.transformersurgeon.qwen2_5_vl_c.modeling_qwen2_5_vl_c &mdash; transformer-surgeon 0.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />

  
    <link rel="canonical" href="https://CESOIA.github.io/transformer-surgeon/_modules/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c.html" />
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=938c9ccc"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            transformer-surgeon
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quickstart.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/index.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">transformer-surgeon</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">transformer-surgeon.transformersurgeon.qwen2_5_vl_c.modeling_qwen2_5_vl_c</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for transformer-surgeon.transformersurgeon.qwen2_5_vl_c.modeling_qwen2_5_vl_c</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="c1">#</span>
<span class="c1"># Copyright 2025 The CESOIA project team, Politecnico di Torino and King Abdullah University of Science and Technology. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This file is based on HuggingFace Transformers Qwen2.5 implementation:</span>
<span class="c1"># Source repository: https://github.com/huggingface/transformers</span>
<span class="c1"># Source file: https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py</span>
<span class="c1"># Commit: 1d742644c09928d6d596c56eae2ffcc8e303be6e</span>
<span class="c1"># Retrieved on 2025-09-03</span>
<span class="c1">#</span>
<span class="c1"># Copyright 2025 The Qwen team and the HuggingFace Inc. team. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This code is based on EleutherAI&#39;s GPT-NeoX library and the GPT-NeoX</span>
<span class="c1"># and OPT implementations in this library. It has been modified from its</span>
<span class="c1"># original forms to accommodate minor architectural differences compared</span>
<span class="c1"># to GPT-NeoX and OPT used by the Meta AI team that trained the model.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># from ...activations import ACT2FN</span>
<span class="c1"># from ...cache_utils import Cache, DynamicCache</span>
<span class="c1"># from ...generation import GenerationMixin</span>
<span class="c1"># from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask</span>
<span class="c1"># from ...modeling_flash_attention_utils import FlashAttentionKwargs</span>
<span class="c1"># from ...modeling_layers import GradientCheckpointingLayer</span>
<span class="c1"># from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput</span>
<span class="c1"># from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update</span>
<span class="c1"># from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel</span>
<span class="c1"># from ...processing_utils import Unpack</span>
<span class="c1"># from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging</span>
<span class="c1"># from ...utils.deprecation import deprecate_kwarg</span>
<span class="c1"># from ..qwen2.modeling_qwen2 import Qwen2RMSNorm</span>
<span class="c1"># from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig</span>
<span class="c1"># -------------</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.activations</span><span class="w"> </span><span class="kn">import</span> <span class="n">ACT2FN</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">auto_docstring</span><span class="p">,</span> <span class="n">logging</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.qwen2.modeling_qwen2</span><span class="w"> </span><span class="kn">import</span> <span class="n">Qwen2RMSNorm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.configuration_qwen2_5_vl_c</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Qwen2_5_VLConfigCompress</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLTextConfigCompress</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLVisionConfigCompress</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">transformers.models.qwen2_5_vl.modeling_qwen2_5_vl</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Qwen2_5_VisionPatchEmbed</span><span class="p">,</span>
    <span class="n">Qwen2_5_VisionRotaryEmbedding</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLPatchMerger</span><span class="p">,</span>
    <span class="c1"># rotate_half,</span>
    <span class="c1"># apply_rotary_pos_emb_vision,</span>
    <span class="c1"># repeat_kv,</span>
    <span class="c1"># eager_attention_forward,</span>
    <span class="n">Qwen2_5_VLVisionAttention</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLVisionBlock</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLPreTrainedModel</span><span class="p">,</span>
    <span class="n">Qwen2_5_VisionTransformerPretrainedModel</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLModelOutputWithPast</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLRotaryEmbedding</span><span class="p">,</span>
    <span class="n">apply_multimodal_rotary_pos_emb</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLAttention</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLDecoderLayer</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLTextModel</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLModel</span><span class="p">,</span>
    <span class="n">Qwen2_5_VLForConditionalGeneration</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">..utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">LinearCompressed</span><span class="p">,</span>
    <span class="n">get_validated_dict_value</span>
<span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLMLP(nn.Module):</span>
<span class="c1">#     def __init__(self, config, bias: bool = False):</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLMLPCompress</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)</span>
        <span class="c1"># self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=bias)</span>
        <span class="c1"># self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=bias)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_gate&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_up&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_down&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">up_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">down_rank</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">))</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VisionPatchEmbed(nn.Module):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VisionRotaryEmbedding(nn.Module):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># NOTE: the next class contains linear layers that could be potentially compressed</span>
<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLPatchMerger(nn.Module):</span>
<span class="c1">#     def __init__(self, dim: int, context_dim: int, spatial_merge_size: int = 2) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.hidden_size = context_dim * (spatial_merge_size**2)</span>
<span class="c1">#         self.ln_q = Qwen2RMSNorm(context_dim, eps=1e-6)</span>
<span class="c1">#         self.mlp = nn.Sequential(</span>
<span class="c1">#             nn.Linear(self.hidden_size, self.hidden_size),</span>
<span class="c1">#             nn.GELU(),</span>
<span class="c1">#             nn.Linear(self.hidden_size, dim),</span>
<span class="c1">#         )</span>

<span class="c1">#     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</span>
<span class="c1">#         x = self.mlp(self.ln_q(x).view(-1, self.hidden_size))</span>
<span class="c1">#         return x</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def rotate_half(x):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def apply_rotary_pos_emb_vision(</span>
<span class="c1"># q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor</span>
<span class="c1"># ) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -&gt; torch.Tensor:</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def eager_attention_forward(</span>
<span class="c1">#     module: nn.Module,</span>
<span class="c1">#     query: torch.Tensor,</span>
<span class="c1">#     key: torch.Tensor,</span>
<span class="c1">#     value: torch.Tensor,</span>
<span class="c1">#     attention_mask: Optional[torch.Tensor],</span>
<span class="c1">#     scaling: float,</span>
<span class="c1">#     dropout: float = 0.0,</span>
<span class="c1">#     **kwargs,</span>
<span class="c1"># ):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLVisionAttention(nn.Module):</span>
<span class="c1">#     def __init__(self, config: Qwen2_5_VLVisionConfig) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.dim = config.hidden_size</span>
<span class="c1">#         self.num_heads = config.num_heads</span>
<span class="c1">#         self.head_dim = self.dim // self.num_heads</span>
<span class="c1">#         self.num_key_value_groups = 1  # needed for eager attention</span>
<span class="c1">#         self.qkv = nn.Linear(self.dim, self.dim * 3, bias=True)</span>
<span class="c1">#         self.proj = nn.Linear(self.dim, self.dim)</span>
<span class="c1">#         self.scaling = self.head_dim**-0.5</span>
<span class="c1">#         self.config = config</span>
<span class="c1">#         self.attention_dropout = 0.0</span>
<span class="c1">#         self.is_causal = False</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLVisionAttentionCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLVisionAttention</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLVisionConfigCompress</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLVisionAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># needed for eager attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_qkv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_proj&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">qkv_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">proj_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,-</span>
    <span class="c1">#     cu_seqlens: torch.Tensor,</span>
    <span class="c1">#     rotary_pos_emb: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ) -&gt; torch.Tensor:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):</span>
<span class="c1">#     def __init__(self, config, attn_implementation: str = &quot;sdpa&quot;) -&gt; None:</span>
<span class="c1">#         super().__init__()</span>
<span class="c1">#         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)</span>
<span class="c1">#         self.norm2 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)</span>
<span class="c1">#         self.attn = Qwen2_5_VLVisionAttention(config=config)</span>
<span class="c1">#         self.mlp = Qwen2_5_VLMLP(config, bias=True)</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLVisionBlockCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLVisionBlock</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sdpa&quot;</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLVisionBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Qwen2_5_VLVisionAttentionCompress</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Qwen2_5_VLMLPCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     cu_seqlens: torch.Tensor,</span>
    <span class="c1">#     rotary_pos_emb: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ) -&gt; torch.Tensor:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2_5_VLPreTrainedModel(PreTrainedModel):</span>
<span class="c1">#     config: Qwen2_5_VLConfig</span>
<span class="c1">#     base_model_prefix = &quot;model&quot;</span>
<span class="c1">#     supports_gradient_checkpointing = True</span>
<span class="c1">#     _no_split_modules = [&quot;Qwen2_5_VLDecoderLayer&quot;, &quot;Qwen2_5_VLVisionBlock&quot;]</span>
<span class="c1">#     _skip_keys_device_placement = &quot;past_key_values&quot;</span>
<span class="c1">#     _supports_flash_attn = True</span>
<span class="c1">#     _supports_sdpa = True</span>

<span class="c1">#     _can_compile_fullgraph = True</span>
<span class="c1">#     _supports_attention_backend = True</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2_5_VLPreTrainedModelCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLPreTrainedModelCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLPreTrainedModelCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLPreTrainedModel</span><span class="p">):</span>
<div class="viewcode-block" id="Qwen2_5_VLPreTrainedModelCompress.config">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLPreTrainedModelCompress.config">[docs]</a>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLConfigCompress</span></div>

<div class="viewcode-block" id="Qwen2_5_VLPreTrainedModelCompress._no_split_modules">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLPreTrainedModelCompress._no_split_modules">[docs]</a>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2_5_VLDecoderLayerCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2_5_VLVisionBlockCompress&quot;</span><span class="p">]</span></div>
</div>

<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VisionTransformerPretrainedModel(Qwen2_5_VLPreTrainedModel):</span>
<span class="c1">#     config: Qwen2_5_VLVisionConfig</span>
<span class="c1">#     _no_split_modules = [&quot;Qwen2_5_VLVisionBlock&quot;]</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VisionTransformerPretrainedModelCompress</span><span class="p">(</span><span class="n">Qwen2_5_VisionTransformerPretrainedModel</span><span class="p">,</span> <span class="n">Qwen2_5_VLPreTrainedModelCompress</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLVisionConfigCompress</span>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2_5_VLVisionBlockCompress&quot;</span><span class="p">]</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_merge_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">spatial_merge_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">patch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fullatt_block_indexes</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">fullatt_block_indexes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_merge_unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_merge_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_merge_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">Qwen2_5_VisionPatchEmbed</span><span class="p">(</span>
            <span class="n">patch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">temporal_patch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">temporal_patch_size</span><span class="p">,</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">embed_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_pos_emb</span> <span class="o">=</span> <span class="n">Qwen2_5_VisionRotaryEmbedding</span><span class="p">(</span><span class="n">head_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># original ---&gt;</span>
        <span class="c1"># self.blocks = nn.ModuleList([Qwen2_5_VLVisionBlock(config) for _ in range(config.depth)])</span>
        <span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">Qwen2_5_VLVisionBlockCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">depth</span><span class="p">)])</span>
        <span class="c1"># &lt;--- CESOIA modifications</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">merger</span> <span class="o">=</span> <span class="n">Qwen2_5_VLPatchMerger</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">out_hidden_size</span><span class="p">,</span>
            <span class="n">context_dim</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">spatial_merge_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">spatial_merge_size</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def rot_pos_emb(self, grid_thw):</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_window_index(self, grid_thw):</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -&gt; torch.Tensor:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @dataclass</span>
<span class="c1"># @auto_docstring(</span>
<span class="c1">#     custom_intro=&quot;&quot;&quot;</span>
<span class="c1">#     Base class for Llava outputs, with hidden states and attentions.</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1"># )</span>
<span class="c1"># class Qwen2_5_VLModelOutputWithPast(ModelOutput):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLRotaryEmbedding(nn.Module):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2MLP(nn.Module):</span>
<span class="c1">#    def __init__(self, config):</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2MLPCompress</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)</span>
        <span class="c1"># self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)</span>
        <span class="c1"># self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_gate&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_up&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;mlp_down&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">up_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">down_rank</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">down_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">down_proj</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim=1):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLAttention(nn.Module):</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     Multi-headed attention from &#39;Attention Is All You Need&#39; paper. Modified to use sliding window attention: Longformer</span>
<span class="c1">#     and &quot;Generating Long Sequences with Sparse Transformers&quot;.</span>
<span class="c1">#     &quot;&quot;&quot;</span>

<span class="c1">#     def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLAttentionCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLAttention</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLTextConfigCompress</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
        <span class="k">if</span> <span class="n">layer_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Instantiating </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> without passing `layer_idx` is not recommended and will &quot;</span>
                <span class="s2">&quot;to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` &quot;</span>
                <span class="s2">&quot;when creating this class.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_causal</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rope_scaling</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rope_scaling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;hidden_size must be divisible by num_heads (got `hidden_size`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; and `num_heads`: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)</span>
        <span class="c1"># self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)</span>
        <span class="c1"># self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)</span>
        <span class="c1"># self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_q&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_k&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_v&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_rank</span> <span class="o">=</span> <span class="n">get_validated_dict_value</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">lrd_rank_lists</span><span class="p">,</span> <span class="s2">&quot;sa_out&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">layer_idx</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">q_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">k_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">v_rank</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">LinearCompressed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lrd_rank</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">o_rank</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sliding_window</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">sliding_window</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;sliding_attention&quot;</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">Qwen2_5_VLRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># @deprecate_kwarg(&quot;past_key_value&quot;, new_name=&quot;past_key_values&quot;, version=&quot;4.58&quot;)</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     output_attentions: bool = False,</span>
    <span class="c1">#     use_cache: bool = False,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC</span>
    <span class="c1">#     **kwargs: Unpack[FlashAttentionKwargs],</span>
    <span class="c1"># ) -&gt; tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLDecoderLayer(GradientCheckpointingLayer):</span>
<span class="c1">#     def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):</span>
<span class="c1">#         super().__init__()</span>
<span class="c1"># -------------</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLDecoderLayerCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLDecoderLayer</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLTextConfigCompress</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>

        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_sliding_window</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">!=</span> <span class="s2">&quot;flash_attention_2&quot;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning_once</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Sliding Window Attention is enabled but not implemented for `</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span><span class="si">}</span><span class="s2">`; &quot;</span>
                <span class="s2">&quot;unexpected results may be encountered.&quot;</span>
            <span class="p">)</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># self.self_attn = Qwen2_5_VLAttention(config, layer_idx)</span>

        <span class="c1"># self.mlp = Qwen2MLP(config)</span>
<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">Qwen2_5_VLAttentionCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Qwen2MLPCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_type</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">layer_types</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># @deprecate_kwarg(&quot;past_key_value&quot;, new_name=&quot;past_key_values&quot;, version=&quot;4.58&quot;)</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     hidden_states: torch.Tensor,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[tuple[torch.Tensor]] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = False,</span>
    <span class="c1">#     use_cache: Optional[bool] = False,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC</span>
    <span class="c1">#     **kwargs: Unpack[FlashAttentionKwargs],</span>
    <span class="c1"># ) -&gt; tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
    
<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2_5_VLTextModel(Qwen2_5_VLPreTrainedModel):</span>
<span class="c1">#     config: Qwen2_5_VLTextConfig</span>

<span class="c1">#     def __init__(self, config: Qwen2_5_VLTextConfig):</span>
<span class="c1">#         super().__init__(config)</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLTextModelCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLTextModel</span><span class="p">,</span> <span class="n">Qwen2_5_VLPreTrainedModelCompress</span><span class="p">):</span>
<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.config">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.config">[docs]</a>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLTextConfigCompress</span></div>


    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLTextConfigCompress</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLTextModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.padding_idx">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.padding_idx">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span></div>

<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.vocab_size">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.vocab_size">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span></div>


<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.embed_tokens">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.embed_tokens">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span></div>


<span class="c1"># original ---&gt;</span>
<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.layers">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.layers">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">Qwen2_5_VLDecoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)]</span>
        <span class="p">)</span></div>

<span class="c1"># -------------</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">Qwen2_5_VLDecoderLayerCompress</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">layer_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">layer_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)]</span>
        <span class="p">)</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress._attn_implementation">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress._attn_implementation">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attn_implementation</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">_attn_implementation</span></div>

<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.norm">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.norm">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">Qwen2RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">)</span></div>

<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.rotary_emb">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.rotary_emb">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">Qwen2_5_VLRotaryEmbedding</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span></div>

<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.has_sliding_layers">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.has_sliding_layers">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_sliding_layers</span> <span class="o">=</span> <span class="s2">&quot;sliding_attention&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">layer_types</span></div>


<div class="viewcode-block" id="Qwen2_5_VLTextModelCompress.gradient_checkpointing">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLTextModelCompress.gradient_checkpointing">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">=</span> <span class="kc">False</span></div>

        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     inputs_embeds: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     use_cache: Optional[bool] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = None,</span>
    <span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
    <span class="c1">#     return_dict: Optional[bool] = None,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     **kwargs: Unpack[FlashAttentionKwargs],</span>
    <span class="c1"># ) -&gt; Union[tuple, BaseModelOutputWithPast]:</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @auto_docstring</span>
<span class="c1"># class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2_5_VLModelCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLModelCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLModel</span><span class="p">,</span> <span class="n">Qwen2_5_VLPreTrainedModelCompress</span><span class="p">):</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<div class="viewcode-block" id="Qwen2_5_VLModelCompress.base_model_prefix">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress.base_model_prefix">[docs]</a>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span></div>

<div class="viewcode-block" id="Qwen2_5_VLModelCompress._checkpoint_conversion_mapping">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress._checkpoint_conversion_mapping">[docs]</a>
    <span class="n">_checkpoint_conversion_mapping</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;^model&quot;</span><span class="p">:</span> <span class="s2">&quot;language_model&quot;</span><span class="p">}</span></div>

    <span class="c1"># Reference: fix gemma3 grad acc #37208</span>
<div class="viewcode-block" id="Qwen2_5_VLModelCompress.accepts_loss_kwargs">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress.accepts_loss_kwargs">[docs]</a>
    <span class="n">accepts_loss_kwargs</span> <span class="o">=</span> <span class="kc">False</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># config: Qwen2_5_VLConfig</span>
    <span class="c1"># _no_split_modules = [&quot;Qwen2_5_VLDecoderLayer&quot;, &quot;Qwen2_5_VLVisionBlock&quot;]</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2_5_VLModelCompress.config">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress.config">[docs]</a>
    <span class="n">config</span><span class="p">:</span> <span class="n">Qwen2_5_VLConfigCompress</span></div>

<div class="viewcode-block" id="Qwen2_5_VLModelCompress._no_split_modules">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress._no_split_modules">[docs]</a>
    <span class="n">_no_split_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2_5_VLDecoderLayerCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2_5_VLVisionBlockCompress&quot;</span><span class="p">]</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># super().__init__(config)</span>
        <span class="c1"># self.visual = Qwen2_5_VisionTransformerPretrainedModel._from_config(config.vision_config)</span>
        <span class="c1"># self.language_model = Qwen2_5_VLTextModel._from_config(config.text_config)</span>
<span class="c1"># -------------</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<div class="viewcode-block" id="Qwen2_5_VLModelCompress.visual">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress.visual">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">visual</span> <span class="o">=</span> <span class="n">Qwen2_5_VisionTransformerPretrainedModelCompress</span><span class="o">.</span><span class="n">_from_config</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vision_config</span><span class="p">)</span></div>

<div class="viewcode-block" id="Qwen2_5_VLModelCompress.language_model">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress.language_model">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span> <span class="o">=</span> <span class="n">Qwen2_5_VLTextModelCompress</span><span class="o">.</span><span class="n">_from_config</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="p">)</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>
<div class="viewcode-block" id="Qwen2_5_VLModelCompress.rope_deltas">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLModelCompress.rope_deltas">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">rope_deltas</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># cache rope_deltas here</span></div>


        <span class="c1"># Initialize weights and apply final processing</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_input_embeddings(self):</span>
    <span class="c1">#     return self.language_model.get_input_embeddings()</span>

    <span class="c1"># def set_input_embeddings(self, value):</span>
    <span class="c1">#     self.language_model.set_input_embeddings(value)</span>

    <span class="c1"># def set_decoder(self, decoder):</span>
    <span class="c1">#     self.language_model = decoder</span>

    <span class="c1"># def get_decoder(self):</span>
    <span class="c1">#     return self.language_model</span>

    <span class="c1"># def get_rope_index(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     image_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     video_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     second_per_grid_ts: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1"># ) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_video_features(</span>
    <span class="c1">#     self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None</span>
    <span class="c1"># ):</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_placeholder_mask(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: torch.LongTensor,</span>
    <span class="c1">#     inputs_embeds: torch.FloatTensor,</span>
    <span class="c1">#     image_features: torch.FloatTensor = None,</span>
    <span class="c1">#     video_features: torch.FloatTensor = None,</span>
    <span class="c1"># ):</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: torch.LongTensor = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     inputs_embeds: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     use_cache: Optional[bool] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = None,</span>
    <span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
    <span class="c1">#     return_dict: Optional[bool] = None,</span>
    <span class="c1">#     pixel_values: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     pixel_values_videos: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     image_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     video_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     rope_deltas: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     second_per_grid_ts: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     **kwargs: Unpack[TransformersKwargs],</span>
    <span class="c1"># ) -&gt; Union[tuple, Qwen2_5_VLModelOutputWithPast]:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># @dataclass</span>
<span class="c1"># @auto_docstring(</span>
<span class="c1">#     custom_intro=&quot;&quot;&quot;</span>
<span class="c1">#     Base class for Qwen2_5_VL causal language model (or autoregressive) outputs.</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1"># )</span>
<span class="c1"># class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):</span>
<span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># NOTE the output layer could be potentially compressed if we take into account the vocabulary we need for a specific task</span>
<span class="c1"># original ---&gt;</span>
<span class="c1"># class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):</span>
<span class="c1"># _checkpoint_conversion_mapping = {</span>
<span class="c1">#         &quot;^visual&quot;: &quot;model.visual&quot;,</span>
<span class="c1">#         r&quot;^model(?!\.(language_model|visual))&quot;: &quot;model.language_model&quot;,</span>
<span class="c1">#     }</span>
<span class="c1">#     _tied_weights_keys = [&quot;lm_head.weight&quot;]</span>
<span class="c1">#     # Reference: fix gemma3 grad acc #37208</span>
<span class="c1">#     accepts_loss_kwargs = False</span>
<span class="c1"># -------------</span>
<div class="viewcode-block" id="Qwen2_5_VLForConditionalGenerationCompress">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLForConditionalGenerationCompress">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Qwen2_5_VLForConditionalGenerationCompress</span><span class="p">(</span><span class="n">Qwen2_5_VLForConditionalGeneration</span><span class="p">,</span> <span class="n">Qwen2_5_VLPreTrainedModelCompress</span><span class="p">):</span>
<div class="viewcode-block" id="Qwen2_5_VLForConditionalGenerationCompress.config_class">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLForConditionalGenerationCompress.config_class">[docs]</a>
    <span class="n">config_class</span> <span class="o">=</span> <span class="n">Qwen2_5_VLConfigCompress</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="c1"># original ---&gt;</span>
        <span class="c1"># super().__init__(config)</span>
        <span class="c1"># self.model = Qwen2_5_VLModel(config)</span>
        <span class="c1"># self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)</span>
<span class="c1"># -------------</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Qwen2_5_VLForConditionalGeneration</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<div class="viewcode-block" id="Qwen2_5_VLForConditionalGenerationCompress.model">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLForConditionalGenerationCompress.model">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">Qwen2_5_VLModelCompress</span><span class="p">(</span><span class="n">config</span><span class="p">)</span></div>

<span class="c1"># &lt;--- CESOIA modifications</span>

<div class="viewcode-block" id="Qwen2_5_VLForConditionalGenerationCompress.lm_head">
<a class="viewcode-back" href="../../../../api/transformer-surgeon/transformersurgeon/qwen2_5_vl_c/modeling_qwen2_5_vl_c/index.html#transformer-surgeon.transformersurgeon.Qwen2_5_VLForConditionalGenerationCompress.lm_head">[docs]</a>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">text_config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></div>


        <span class="bp">self</span><span class="o">.</span><span class="n">post_init</span><span class="p">()</span></div>


<span class="c1"># original ---&gt;</span>
    <span class="c1"># def get_input_embeddings(self):</span>
    <span class="c1">#     return self.model.get_input_embeddings()</span>

    <span class="c1"># def set_input_embeddings(self, value):</span>
    <span class="c1">#     self.model.set_input_embeddings(value)</span>

    <span class="c1"># def set_decoder(self, decoder):</span>
    <span class="c1">#     self.model.set_decoder(decoder)</span>

    <span class="c1"># def get_decoder(self):</span>
    <span class="c1">#     return self.model.get_decoder()</span>

    <span class="c1"># def get_video_features(</span>
    <span class="c1">#     self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None</span>
    <span class="c1"># ):</span>
    <span class="c1">#     return self.model.get_video_features(pixel_values_videos, video_grid_thw)</span>

    <span class="c1"># def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):</span>
    <span class="c1">#     return self.model.get_image_features(pixel_values, image_grid_thw)</span>


    <span class="c1"># # Make modules available through conditional class for BC</span>
    <span class="c1"># @property</span>
    <span class="c1"># def language_model(self):</span>
    <span class="c1">#     return self.model.language_model</span>

    <span class="c1"># @property</span>
    <span class="c1"># def visual(self):</span>
    <span class="c1">#     return self.model.visual</span>

    <span class="c1"># @can_return_tuple</span>
    <span class="c1"># @auto_docstring</span>
    <span class="c1"># def forward(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: torch.LongTensor = None,</span>
    <span class="c1">#     attention_mask: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     position_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     past_key_values: Optional[Cache] = None,</span>
    <span class="c1">#     inputs_embeds: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     labels: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     use_cache: Optional[bool] = None,</span>
    <span class="c1">#     output_attentions: Optional[bool] = None,</span>
    <span class="c1">#     output_hidden_states: Optional[bool] = None,</span>
    <span class="c1">#     pixel_values: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     pixel_values_videos: Optional[torch.FloatTensor] = None,</span>
    <span class="c1">#     image_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     video_grid_thw: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     rope_deltas: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     cache_position: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     second_per_grid_ts: Optional[torch.Tensor] = None,</span>
    <span class="c1">#     logits_to_keep: Union[int, torch.Tensor] = 0,</span>
    <span class="c1">#     **kwargs: Unpack[TransformersKwargs],</span>
    <span class="c1"># ) -&gt; Union[tuple, Qwen2_5_VLCausalLMOutputWithPast]:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def prepare_inputs_for_generation(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids,</span>
    <span class="c1">#     past_key_values=None,</span>
    <span class="c1">#     attention_mask=None,</span>
    <span class="c1">#     inputs_embeds=None,</span>
    <span class="c1">#     cache_position=None,</span>
    <span class="c1">#     position_ids=None,</span>
    <span class="c1">#     use_cache=True,</span>
    <span class="c1">#     pixel_values=None,</span>
    <span class="c1">#     pixel_values_videos=None,</span>
    <span class="c1">#     image_grid_thw=None,</span>
    <span class="c1">#     video_grid_thw=None,</span>
    <span class="c1">#     second_per_grid_ts=None,</span>
    <span class="c1">#     **kwargs,</span>
    <span class="c1"># ):</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def _get_image_nums_and_video_nums(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor],</span>
    <span class="c1">#     inputs_embeds: Optional[torch.Tensor] = None,</span>
    <span class="c1"># ) -&gt; tuple[torch.Tensor, torch.Tensor]:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
    <span class="c1"># def _expand_inputs_for_generation(</span>
    <span class="c1">#     self,</span>
    <span class="c1">#     expand_size: int = 1,</span>
    <span class="c1">#     is_encoder_decoder: bool = False,</span>
    <span class="c1">#     input_ids: Optional[torch.LongTensor] = None,</span>
    <span class="c1">#     **model_kwargs,</span>
    <span class="c1"># ) -&gt; tuple[torch.LongTensor, dict[str, Any]]:</span>
    <span class="c1"># [...]</span>
<span class="c1"># -------------</span>
<span class="c1"># &lt;--- CESOIA modifications</span>

<span class="c1"># original ---&gt;</span>
<span class="c1"># __all__ = [&quot;Qwen2_5_VLForConditionalGeneration&quot;, &quot;Qwen2_5_VLModel&quot;, &quot;Qwen2_5_VLPreTrainedModel&quot;, &quot;Qwen2_5_VLTextModel&quot;]</span>
<span class="c1"># -------------</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Qwen2_5_VLForConditionalGenerationCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2_5_VLModelCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2_5_VLPreTrainedModelCompress&quot;</span><span class="p">,</span> <span class="s2">&quot;Qwen2_5_VLTextModelCompress&quot;</span><span class="p">]</span>
<span class="c1"># &lt;--- CESOIA modifications</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, CESOIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>